{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5451e8f",
   "metadata": {},
   "source": [
    "1. Pre-process the input data \n",
    "\n",
    "\n",
    "Reference:\n",
    "\n",
    "https://medium.com/@bijil.subhash/code-walkthrough-of-word2vec-pytorch-implementation-3a9ca0ad55a7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ed96bcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f94e3354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>ffe987279560d7ff</td>\n",
       "      <td>\":::::And for the second time of asking, when ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>ffea4adeee384e90</td>\n",
       "      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>ffee36eab5c267c9</td>\n",
       "      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>fff125370e4aaaf3</td>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>fff46fc426af1f9a</td>\n",
       "      <td>\"\\nAnd ... I really don't think you understand...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159571 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "0       0000997932d777bf  Explanation\\nWhy the edits made under my usern...   \n",
       "1       000103f0d9cfb60f  D'aww! He matches this background colour I'm s...   \n",
       "2       000113f07ec002fd  Hey man, I'm really not trying to edit war. It...   \n",
       "3       0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...   \n",
       "4       0001d958c54c6e35  You, sir, are my hero. Any chance you remember...   \n",
       "...                  ...                                                ...   \n",
       "159566  ffe987279560d7ff  \":::::And for the second time of asking, when ...   \n",
       "159567  ffea4adeee384e90  You should be ashamed of yourself \\n\\nThat is ...   \n",
       "159568  ffee36eab5c267c9  Spitzer \\n\\nUmm, theres no actual article for ...   \n",
       "159569  fff125370e4aaaf3  And it looks like it was actually you who put ...   \n",
       "159570  fff46fc426af1f9a  \"\\nAnd ... I really don't think you understand...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0           0             0        0       0       0              0  \n",
       "1           0             0        0       0       0              0  \n",
       "2           0             0        0       0       0              0  \n",
       "3           0             0        0       0       0              0  \n",
       "4           0             0        0       0       0              0  \n",
       "...       ...           ...      ...     ...     ...            ...  \n",
       "159566      0             0        0       0       0              0  \n",
       "159567      0             0        0       0       0              0  \n",
       "159568      0             0        0       0       0              0  \n",
       "159569      0             0        0       0       0              0  \n",
       "159570      0             0        0       0       0              0  \n",
       "\n",
       "[159571 rows x 8 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = '/Users/irsaashraf/Desktop/UChicago/Spring_23/Advanced ML/Project/Irsa_project/train.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "972592af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_file_path, transform=None):\n",
    "        self.data = pd.read_csv(data_file_path)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data.iloc[index]\n",
    "        # Extract the relevant columns from the CSV file\n",
    "        text = sample[\"comment_text\"]\n",
    "        label = sample[\"toxic\"]\n",
    "        # Apply any transformations to the data\n",
    "        if self.transform:\n",
    "            text, label = self.transform(text, label)\n",
    "        return text, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c56253",
   "metadata": {},
   "source": [
    "Apply transformation to dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d99b095d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CustomDataset at 0x7fea829b4b50>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = CustomDataset(file_path)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3e6ad79f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159571"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "041dfc45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.CustomDataset"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5352e381",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_lst = [] \n",
    "label_lst = []\n",
    "for i in range(len(train_data) - 1):\n",
    "    text, label = train_data[i]\n",
    "    text_lst.append(text)\n",
    "    label_lst.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd8f873",
   "metadata": {},
   "source": [
    "### Tokenize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "035e75dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(train_iter):\n",
    "    '''\n",
    "    Documentation:\n",
    "    https://pytorch.org/text/stable/_modules/torchtext/data/utils.html#get_tokenizer\n",
    "    Reference:\n",
    "    https://medium.com/@bijil.subhash/code-walkthrough-of-word2vec-pytorch-implementation-3a9ca0ad55a7\n",
    "    '''\n",
    "    tokenizer = get_tokenizer(\"basic_english\", language=\"en\")\n",
    "    for text, labels in train_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "# def get_english_tokenizer():\n",
    "#     \"\"\"\n",
    "#     Documentation:\n",
    "#     https://pytorch.org/text/stable/_modules/torchtext/data/utils.html#get_tokenizer\n",
    "#     Reference:\n",
    "#     https://medium.com/@bijil.subhash/code-walkthrough-of-word2vec-pytorch-implementation-3a9ca0ad55a7\n",
    "#     \"\"\"\n",
    "#     tokenizer = get_tokenizer(\"basic_english\", language=\"en\")\n",
    "#     return tokenizer\n",
    "\n",
    "\n",
    "def build_vocab(data_iter, MIN_WORD_FREQUENCY):\n",
    "    \"\"\"\n",
    "    Builds vocabulary from iterator\n",
    "    \"\"\"\n",
    "    vocab = build_vocab_from_iterator(yield_tokens(data_iter), specials=[\"<unk>\"], min_freq=MIN_WORD_FREQUENCY)\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f4828937",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_WORD_FREQUENCY = 1000\n",
    "\n",
    "vocab = build_vocab(train_data, MIN_WORD_FREQUENCY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b01ba7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To view the token to index mapping \n",
    "\n",
    "# vocab.get_stoi()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05242ccf",
   "metadata": {},
   "source": [
    "### Create utility functions for collating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "03a18068",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 256\n",
    "CBOW_N_WORDS = 4\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\", language=\"en\")\n",
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "\n",
    "\n",
    "def collate_cbow(batch, text_pipeline):\n",
    "    \"\"\"\n",
    "    https://medium.com/@bijil.subhash/code-walkthrough-of-word2vec-pytorch-implementation-3a9ca0ad55a7\n",
    "    \n",
    "    Collate_fn for CBOW model to be used with Dataloader.\n",
    "    `batch` is expected to be list of text paragrahs.\n",
    "    \n",
    "    Context is represented as N=CBOW_N_WORDS past words \n",
    "    and N=CBOW_N_WORDS future words.\n",
    "    \n",
    "    Long paragraphs will be truncated to contain\n",
    "    no more that MAX_SEQUENCE_LENGTH tokens.\n",
    "    \n",
    "    Each element in `batch_input` is N=CBOW_N_WORDS*2 context words.\n",
    "    Each element in `batch_output` is a middle word.\n",
    "    \"\"\"\n",
    "    batch_input, batch_output = [], []\n",
    "    for text, labels in batch:\n",
    "        text_tokens_ids = text_pipeline(text)\n",
    "\n",
    "        if len(text_tokens_ids) < CBOW_N_WORDS * 2 + 1:\n",
    "            continue\n",
    "\n",
    "        if MAX_SEQUENCE_LENGTH:\n",
    "            text_tokens_ids = text_tokens_ids[:MAX_SEQUENCE_LENGTH]\n",
    "\n",
    "        for idx in range(len(text_tokens_ids) - CBOW_N_WORDS * 2):\n",
    "            token_id_sequence = text_tokens_ids[idx : (idx + CBOW_N_WORDS * 2 + 1)]\n",
    "            output = token_id_sequence.pop(CBOW_N_WORDS)\n",
    "            input_ = token_id_sequence\n",
    "            batch_input.append(input_)\n",
    "            batch_output.append(output)\n",
    "\n",
    "    batch_input = torch.tensor(batch_input, dtype=torch.long)\n",
    "    batch_output = torch.tensor(batch_output, dtype=torch.long)\n",
    "    return batch_input, batch_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f6ff27ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## DELETE ## \n",
    "\n",
    "# for text, labels in train_data:\n",
    "#     text_tokens_ids = text_pipeline(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "9727c41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_tokens_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96e21b5",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c5604031",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from functools import partial\n",
    "\n",
    "batch_size = 16\n",
    "collate_fn = collate_cbow\n",
    "\n",
    "dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=False, \n",
    "                        collate_fn=partial(collate_fn, text_pipeline=text_pipeline))\n",
    "\n",
    "# dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=False, \n",
    "#                         collate_fn=collate_cbow(train_data, text_pipeline))\n",
    "\n",
    "# dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=False, \n",
    "#                         collate_fn=collate_cbow(train_data, text_pipeline))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c12971",
   "metadata": {},
   "source": [
    "### Build Classifier\n",
    "\n",
    "https://www.cse.chalmers.se/~richajo/nlp2019/l2/Text%20classification%20using%20a%20CBoW%20representation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "969fc34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.cse.chalmers.se/~richajo/nlp2019/l2/Text%20classification%20using%20a%20CBoW%20representation.html\n",
    "\n",
    "class CBoWClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, num_labels, dropout=0.5):\n",
    "        super().__init__()   \n",
    "        # Embedding layer: we specify the vocabulary size and embedding dimensionality.        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # A linear output layer.\n",
    "        self.top_layer = nn.Linear(embedding_dim, num_labels)\n",
    "        \n",
    "    def forward(self, docs):\n",
    "        # The words in the documents are encoded as integers. The shape of the documents\n",
    "        # tensor is (max_len, n_docs), where n_docs is the number of documents in this batch,\n",
    "        # and max_len is the maximal length of a document in the batch.\n",
    "\n",
    "        # First look up the embeddings for all the words in the documents.\n",
    "        # The shape is now (max_len, n_docs, emb_dim).\n",
    "        embedded = self.embedding(docs)\n",
    "\n",
    "        # Compute the mean of word embeddings over the documents.\n",
    "        # The shape is now (n_docs, emb_dim)\n",
    "        cbow = embedded.mean(dim=0)\n",
    "\n",
    "        # Apply the dropout layer. (This is only used during training, not during testing.)\n",
    "#         cbow_drop = self.dropout(cbow)\n",
    "\n",
    "        # Finally, compute the output scores.\n",
    "#         scores = self.top_layer(cbow_drop)\n",
    "        scores = self.top_layer(cbow)\n",
    "        \n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5d744c",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f6db81f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=len(vocab)\n",
    "embedding_dim=900\n",
    "num_labels=2\n",
    "\n",
    "model = CBoWClassifier(vocab_size=vocab_size, embedding_dim=embedding_dim, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1ee4a92a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CBoWClassifier(\n",
       "  (embedding): Embedding(1094, 900)\n",
       "  (top_layer): Linear(in_features=900, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "41f4a7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "loss_function = torch.nn.NLLLoss()\n",
    "\n",
    "def train_an_epoch(dataloader, optimizer):\n",
    "    model.train() # Sets the module in training mode.\n",
    "    log_interval = 500\n",
    "    num_nan_batch = 0\n",
    "\n",
    "    for idx, (text, label) in enumerate(dataloader):\n",
    "        model.zero_grad()\n",
    "        log_probs = model(text)\n",
    "\n",
    "        has_nan = torch.isnan(log_probs).any().item()\n",
    "        if has_nan:\n",
    "            num_nan_batch += 1\n",
    "            # print(text)\n",
    "            # print(label)\n",
    "            continue\n",
    "\n",
    "        loss = loss_function(log_probs, label)\n",
    "        print(\"loss: \", loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            print(f'At iteration {idx} the loss is {loss:.3f}.')\n",
    "        if idx % 5000 == 0 and idx > 0:\n",
    "            print(\"Skip\", num_nan_batch, \"batches containing nan probability\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7616a153",
   "metadata": {},
   "source": [
    "### Create functions for accuracy  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "031da18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "9e6136b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(dataloader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        total_count = 0\n",
    "        total_correct = 0\n",
    "        \n",
    "        for idx, (label, text) in enumerate(dataloader):\n",
    "            total_count += len(label)\n",
    "            log_probs = model(text)\n",
    "            preds = torch.argmax(log_probs, dim=1)\n",
    "            total_correct += sum(preds == label)\n",
    "\n",
    "        return total_correct / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "04f03644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data.dataset import random_split\n",
    "# test_data = pd.read_csv('/Users/irsaashraf/Desktop/UChicago/Spring_23/Advanced ML/Project/Irsa_project/test.csv')\n",
    "# test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "18eb7849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_labels = pd.read_csv('/Users/irsaashraf/Desktop/UChicago/Spring_23/Advanced ML/Project/Irsa_project/test_labels.csv')\n",
    "# test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "9a2a9a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "BATCH_SIZE = 8 # batch size for training\n",
    "  \n",
    "# train_valid_data, test_data = AG_NEWS()\n",
    "train_valid_data = list(train_data)\n",
    "num_train = int(len(train_valid_data) * 0.95)\n",
    "num_valid = len(train_valid_data) - num_train\n",
    "train_data, valid_data = random_split(\n",
    "    train_valid_data, [num_train, num_valid])\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, \n",
    "                              collate_fn=partial(collate_fn, text_pipeline=text_pipeline))\n",
    "valid_dataloader = DataLoader(valid_data, batch_size=BATCH_SIZE,\n",
    "                              shuffle=False, \n",
    "                              collate_fn=partial(collate_fn, text_pipeline=text_pipeline))\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE,\n",
    "                             shuffle=False, \n",
    "                             collate_fn=partial(collate_fn, text_pipeline=text_pipeline))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cb3677",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "2ece3993",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (8) to match target batch_size (629).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[159], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, EPOCHS \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      9\u001b[0m     epoch_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 10\u001b[0m     \u001b[43mtrain_an_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m get_accuracy(valid_dataloader)\n\u001b[1;32m     12\u001b[0m     accuracies\u001b[38;5;241m.\u001b[39mappend(accuracy)\n",
      "Cell \u001b[0;32mIn[139], line 21\u001b[0m, in \u001b[0;36mtrain_an_epoch\u001b[0;34m(dataloader, optimizer)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# print(text)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# print(label)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss: \u001b[39m\u001b[38;5;124m\"\u001b[39m, loss)\n\u001b[1;32m     23\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/capp30255/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/capp30255/lib/python3.10/site-packages/torch/nn/modules/loss.py:216\u001b[0m, in \u001b[0;36mNLLLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnll_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/capp30255/lib/python3.10/site-packages/torch/nn/functional.py:2704\u001b[0m, in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2702\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2703\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 2704\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnll_loss_nd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (8) to match target batch_size (629)."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "EPOCHS = 15 # epoch\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=3)\n",
    "\n",
    "accuracies=[]\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train_an_epoch(train_dataloader, optimizer)\n",
    "    accuracy = get_accuracy(valid_dataloader)\n",
    "    accuracies.append(accuracy)\n",
    "    time_taken = time.time() - epoch_start_time\n",
    "    print()\n",
    "    print(f'After epoch {epoch} the validation accuracy is {accuracy:.3f}.')\n",
    "    print()\n",
    "    \n",
    "plt.plot(range(1, EPOCHS+1), accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648f2f3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "14b9a4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_accuracy(dataloader):\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         ## WRITE YOUR CODE BELOW.\n",
    "#         acc = 0\n",
    "#         num_examples = 0\n",
    "#         for idx, (label, text) in enumerate(dataloader):\n",
    "#             acc += torch.sum(label == text.argmax(1))\n",
    "#             num_examples += len(text)\n",
    "#         accuracy = acc/num_examples\n",
    "#         return accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b89dd33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:capp30255]",
   "language": "python",
   "name": "conda-env-capp30255-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
