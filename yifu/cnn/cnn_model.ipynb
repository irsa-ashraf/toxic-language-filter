{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model for Toxic Language Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download(\"all\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "MAX_SENT_LENGTH = 200\n",
    "EMBED_DIM = 300"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1b7fe21f6b20f176</td>\n",
       "      <td>The Wack Pack \\n\\nYou're fucking insane and dr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2b77b61ee27ab6fb</td>\n",
       "      <td>Not according to Wikipedia policy. With respec...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>626d12d910b34c40</td>\n",
       "      <td>You just revert my and a lot of peoples work, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2ed8095b9ee464b0</td>\n",
       "      <td>Frank O'Hara, Anti-Beat? \\nSeriously, dude, yo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>86f8607fd0572b9f</td>\n",
       "      <td>\"\\n\\nreverting\\nUser:Baristarim decides that t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic\n",
       "0  1b7fe21f6b20f176  The Wack Pack \\n\\nYou're fucking insane and dr...      1\n",
       "1  2b77b61ee27ab6fb  Not according to Wikipedia policy. With respec...      0\n",
       "2  626d12d910b34c40  You just revert my and a lot of peoples work, ...      1\n",
       "3  2ed8095b9ee464b0  Frank O'Hara, Anti-Beat? \\nSeriously, dude, yo...      0\n",
       "4  86f8607fd0572b9f  \"\\n\\nreverting\\nUser:Baristarim decides that t...      0"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load balanced data\n",
    "data = pd.read_csv(\"../data/balanced_data.csv\")\n",
    "data = data[data[\"word_count\"] <= MAX_SENT_LENGTH]\n",
    "data = data.reset_index()\n",
    "data = data[[\"id\", \"comment_text\", \"toxic\"]]\n",
    "data.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GPU set up"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pre-trained text embedding set up "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GloVe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2044,  0.1643,  0.0418,  ..., -0.3401, -0.0771, -0.0841],\n",
      "        [-0.1749,  0.2296,  0.2492,  ..., -0.2413, -0.4040,  0.0547],\n",
      "        [-0.2971,  0.0940, -0.0967,  ...,  0.0597, -0.2285,  0.2960],\n",
      "        [ 0.2887, -0.5307, -0.0883,  ..., -0.2334, -0.2379,  0.3161]])\n",
      "torch.Size([4, 300])\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "VECTOR_CACHE_DIR = './glove/.vector_cache'\n",
    "\n",
    "glove = GloVe(name='6B', cache = VECTOR_CACHE_DIR)\n",
    "\n",
    "print(glove.get_vecs_by_tokens([\"this\", \"is\", \"a\", \"comment\"]))\n",
    "print(glove.get_vecs_by_tokens([\"this\", \"is\", \"a\", \"comment\"]).shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenize word and map with embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def process_input(batch):\n",
    "    '''\n",
    "    Tokenize a batch of texts:\n",
    "        - convert sentence to lower case\n",
    "        - tokenize word with package `word_tokenize`\n",
    "        - add paddings to MAX_SENT_LENGTH\n",
    "        - convert tokenized sentence into GloVe embeddings\n",
    "        - return a numpy array of input vectors \n",
    "    '''\n",
    "\n",
    "    # set up containers\n",
    "    y = torch.zeros(BATCH_SIZE)\n",
    "    x = torch.zeros(BATCH_SIZE, MAX_SENT_LENGTH, EMBED_DIM)\n",
    "    \n",
    "    for i, (sent, label) in enumerate(batch):\n",
    "\n",
    "        sent = sent.lower()\n",
    "        tokenized_sent = word_tokenize(sent)\n",
    "\n",
    "        # perform padding or truncate\n",
    "        if len(tokenized_sent) < MAX_SENT_LENGTH:\n",
    "            tokenized_sent += ['<pad>'] * (MAX_SENT_LENGTH - len(tokenized_sent))\n",
    "        else:\n",
    "            tokenized_sent = tokenized_sent[:MAX_SENT_LENGTH]\n",
    "\n",
    "        vecs = glove.get_vecs_by_tokens(tokenized_sent)\n",
    "\n",
    "        x[i] = vecs\n",
    "        y[i] = label\n",
    "    \n",
    "    y = y.type(torch.LongTensor)\n",
    "\n",
    "    return x, y \n",
    "\n",
    "# def process_input(batch):\n",
    "#     '''\n",
    "#     Tokenize a batch of texts:\n",
    "#         - convert sentence to lower case\n",
    "#         - tokenize word with package `word_tokenize`\n",
    "#         - add paddings to MAX_SENT_LENGTH\n",
    "#         - convert tokenized sentence into GloVe embeddings\n",
    "#         - return a numpy array of input vectors \n",
    "#     '''\n",
    "\n",
    "#     # set up containers\n",
    "#     y = torch.zeros(BATCH_SIZE)\n",
    "#     x = torch.zeros(BATCH_SIZE, MAX_SENT_LENGTH)\n",
    "    \n",
    "#     for i, (sent, label) in enumerate(batch):\n",
    "\n",
    "#         sent = sent.lower()\n",
    "#         tokenized_sent = word_tokenize(sent)\n",
    "\n",
    "#         # perform padding or truncate\n",
    "#         if len(tokenized_sent) < MAX_SENT_LENGTH:\n",
    "#             tokenized_sent += ['<pad>'] * (MAX_SENT_LENGTH - len(tokenized_sent))\n",
    "#         else:\n",
    "#             tokenized_sent = tokenized_sent[:MAX_SENT_LENGTH]\n",
    "\n",
    "#         x[i] = torch.tensor(tokenized_sent)\n",
    "#         y[i] = label\n",
    "\n",
    "#     return x, y "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Custome Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customized dataset: \n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    '''\n",
    "    '''\n",
    "    def __init__(self, df):\n",
    "        # self.is_test = is_test\n",
    "        self.dataframe = df\n",
    "        self.label_name = df.columns[-1]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataframe.iloc[idx]['comment_text']\n",
    "        label = self.dataframe.iloc[idx][self.label_name]\n",
    "        return text, label\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data: train 70%, validation 20%, test 10%\n",
    "\n",
    "train_data, rest_data = train_test_split(data, test_size=0.3, random_state=1)\n",
    "val_data, test_data = train_test_split(rest_data, test_size=0.33, random_state=1)\n",
    "\n",
    "# convert dataframes into cusmotized datasets\n",
    "\n",
    "train_data = CustomDataset(train_data)\n",
    "val_data = CustomDataset(val_data)\n",
    "test_data = CustomDataset(test_data)\n",
    "\n",
    "# create dataloader\n",
    "# droplast: if last batch is incomplete, we drop the last batch\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, \n",
    "                              collate_fn=process_input, drop_last=True)\n",
    "\n",
    "val_dataloader = DataLoader(val_data, batch_size=BATCH_SIZE, \n",
    "                            collate_fn=process_input, drop_last=True)\n",
    "\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, \n",
    "                             collate_fn=process_input, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (text, label) in enumerate(test_dataloader):\n",
    "#     print(i)\n",
    "#     print(text)\n",
    "#     print(label)\n",
    "\n",
    "#     if i == 2:\n",
    "#         break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CNN Version 0 </p>\n",
    "\n",
    "For the first model, we are testing the following parameters\n",
    "\n",
    "|Description         |Values           |\n",
    "|:------------------:|:---------------:|\n",
    "|input word vectors  |GloVe            |\n",
    "|embedding size      |300              |\n",
    "|filter sizes        |(2, 3, 4, 5)     |\n",
    "|num filters         |(64, 64, 64, 64) |\n",
    "|activation          |ReLU             |\n",
    "|pooling             |1-max pooling    |\n",
    "|dropout rate        |0.5              |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN_NLP(nn.Module):\n",
    "    '''\n",
    "    An 1D Convulational Neural Network for Sentence Classification.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, filter_sizes=[2, 3, 4, 5], num_filters=[64, 64, 64, 64], \n",
    "                 embed_dim=EMBED_DIM, num_classes=2, dropout=0.5):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            pretrained_embedding (torch.Tensor): size (max_sent_length, embed_dim)\n",
    "            freeze_embedding (bool): set to False by default\n",
    "            max_sent_length (int): longest sentence allowed\n",
    "            embed_dim (int): dimension of word vectors, by default: 300\n",
    "            filter_sizes (List[int]): list of filter sizes, by default: [2, 3, 4, 5]\n",
    "            num_filters (List[int]): list of number of filters, [64] * 4\n",
    "            n_classes (int): number of classes, by default: 2\n",
    "            dropout (float): Dropout rate, by default: 0.5\n",
    "        \"\"\"\n",
    "\n",
    "        super(CNN_NLP, self).__init__()\n",
    "\n",
    "        # # Embedding layer\n",
    "        # if pretrained_embedding is not None:\n",
    "        #     self.vocab_size, self.embed_dim = pretrained_embedding.shape\n",
    "        #     self.embedding = nn.Embedding.from_pretrained(pretrained_embedding,\n",
    "        #                                                   freeze=freeze_embedding)\n",
    "        # else:\n",
    "        #     self.embed_dim = embed_dim\n",
    "        #     self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
    "        #                                   embedding_dim=self.embed_dim,\n",
    "        #                                   padding_idx=0,\n",
    "        #                                   max_norm=5.0)\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Conv Network\n",
    "        self.conv1d_list = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=self.embed_dim, \n",
    "                      out_channels=num_filters[i], \n",
    "                      kernel_size=filter_sizes[i])\n",
    "            for i in range(len(filter_sizes))])\n",
    "        \n",
    "        # Fully-connected layer and Dropout\n",
    "        self.fc = nn.Linear(np.sum(num_filters), num_classes)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Perform a forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            input (torch.Tensor): tensor of batch of sentences with shape\n",
    "            (batch_size, max_sent_length, embedding_dim)\n",
    "\n",
    "        Returns:\n",
    "            logits (torch.Tensor): Output logits with shape (batch_size,\n",
    "            n_classes)\n",
    "        \"\"\"\n",
    "\n",
    "        # permute input to match the shape requirement of `nn.Conv1d`.\n",
    "        # x_reshaped shape: (BATCH_SIZE, EMBED_DIM, MAX_SENT_LENGTH)\n",
    "        x_reshaped = input.permute(0, 2, 1)\n",
    "\n",
    "        # Apply CNN and ReLU. Output shape: (b, num_filters[i], L_out)\n",
    "        x_conv_list = [F.relu(conv1d(x_reshaped)) for conv1d in self.conv1d_list]\n",
    "\n",
    "        # Max pooling. Output shape: (b, num_filters[i], 1)\n",
    "        x_pool_list = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2]) for x_conv in x_conv_list]\n",
    "        \n",
    "        # Concatenate x_pool_list to feed the fully connected layer.\n",
    "        # Output shape: (b, sum(num_filters))\n",
    "        x_fc = torch.cat([x_pool.squeeze(dim=2) for x_pool in x_pool_list], dim=1)\n",
    "        \n",
    "        # Compute logits. Output shape: (b, n_classes)\n",
    "        logits = self.fc(self.dropout(x_fc))\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Check for GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "def initilize_model(embed_dim=EMBED_DIM, filter_sizes=[2, 3, 4, 5], num_filters=[64, 64, 64, 64], \n",
    "                    num_classes=2, dropout=0.5, learning_rate=0.01):\n",
    "\n",
    "    '''\n",
    "    Instantiate a CNN model and an optimizer.\n",
    "    '''\n",
    "\n",
    "    assert (len(filter_sizes) == len(num_filters)), \"filter_sizes and \\\n",
    "    num_filters need to be of the same length.\"\n",
    "\n",
    "    cnn_model = CNN_NLP(filter_sizes=filter_sizes,\n",
    "                        num_filters=num_filters,\n",
    "                        embed_dim=embed_dim,\n",
    "                        num_classes=num_classes,\n",
    "                        dropout=dropout)\n",
    "    \n",
    "    cnn_model.to(device)\n",
    "\n",
    "    optimizer = optim.Adadelta(cnn_model.parameters(),\n",
    "                               lr=learning_rate,\n",
    "                               rho=0.95)\n",
    "\n",
    "    return cnn_model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    '''\n",
    "    Set seed for reproducibility.\n",
    "    '''\n",
    "\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "\n",
    "def evaluate_val(model, val_dataloader):\n",
    "    '''\n",
    "    After the completion of each training epoch, \n",
    "    measure the model's performance on our validation set.\n",
    "    '''\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    for batch in val_dataloader:\n",
    "\n",
    "        b_input, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input)\n",
    "\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "\n",
    "def train(model, optimizer, train_dataloader, val_dataloader, epochs=10):\n",
    "    '''\n",
    "    Train the CNN model.\n",
    "    '''\n",
    "    \n",
    "    best_model = None\n",
    "    val_accuracies = []\n",
    "    val_losses = []\n",
    "    best_accuracy = 0\n",
    "\n",
    "    print(\"Start training...\\n\")\n",
    "    print(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "    print(\"-\"*60)\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "\n",
    "        t0_epoch = time.time()\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        # ============= Training =============\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "            # forward and compute loss\n",
    "            b_input, b_labels = tuple(t.to(device) for t in batch)\n",
    "            model.zero_grad()\n",
    "            logits = model(b_input)\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        time_elapsed = time.time() - t0_epoch\n",
    "\n",
    "        # ============= Evaluation =============\n",
    "\n",
    "        val_loss, val_accuracy = evaluate_val(model, val_dataloader)\n",
    "\n",
    "        if not best_model:\n",
    "            best_model = model\n",
    "\n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            best_model = model\n",
    "\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        time_elapsed = time.time() - t0_epoch\n",
    "        print(f\"{epoch_i + 1:^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "            \n",
    "    print(\"\\n\")\n",
    "    print(f\"Training complete! Best accuracy: {best_accuracy:.2f}%.\")\n",
    "    return best_model, val_accuracies, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   1    |   0.515071   |  0.387810  |   85.50   |  222.95  \n",
      "   2    |   0.357476   |  0.314949  |   87.84   |  214.95  \n",
      "   3    |   0.307458   |  0.286911  |   88.58   |  217.30  \n",
      "   4    |   0.283827   |  0.271694  |   89.29   |  212.41  \n",
      "   5    |   0.269294   |  0.261957  |   89.29   |  217.79  \n",
      "   6    |   0.257123   |  0.254316  |   89.50   |  238.22  \n",
      "   7    |   0.247575   |  0.248300  |   89.76   |  223.09  \n",
      "   8    |   0.239426   |  0.243885  |   89.81   |  216.53  \n",
      "   9    |   0.233904   |  0.239794  |   90.08   |  210.70  \n",
      "  10    |   0.227885   |  0.236559  |   90.17   |  221.15  \n",
      "\n",
      "\n",
      "Training complete! Best accuracy: 90.17%.\n"
     ]
    }
   ],
   "source": [
    "cnn_model, optimizer = initilize_model()\n",
    "best_model_0, val_accuracies_0, val_losses_0 = train(cnn_model, optimizer, train_dataloader, val_dataloader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### REPORT 1:\n",
    "\n",
    "**Model:** `best_model_0` </p>\n",
    "\n",
    "```python\n",
    "\n",
    "(CNN_NLP(\n",
    "   (conv1d_list): ModuleList(\n",
    "     (0): Conv1d(300, 64, kernel_size=(2,), stride=(1,))\n",
    "     (1): Conv1d(300, 64, kernel_size=(3,), stride=(1,))\n",
    "     (2): Conv1d(300, 64, kernel_size=(4,), stride=(1,))\n",
    "     (3): Conv1d(300, 64, kernel_size=(5,), stride=(1,))\n",
    "   )\n",
    "   (fc): Linear(in_features=256, out_features=2, bias=True)\n",
    "   (dropout): Dropout(p=0.5, inplace=False)\n",
    " )\n",
    "```\n",
    "\n",
    "**Training performance:**\n",
    "\n",
    "| Epoch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
    "|:------:|:------------:|:----------:|:---------:|:-------:|\n",
    "|   1    |   0.515071   |  0.387810  |   85.50   |  222.95 | \n",
    "|   2    |   0.357476   |  0.314949  |   87.84   |  214.95 |\n",
    "|   3    |   0.307458   |  0.286911  |   88.58   |  217.30 |\n",
    "|   4    |   0.283827   |  0.271694  |   89.29   |  212.41 |\n",
    "|   5    |   0.269294   |  0.261957  |   89.29   |  217.79 |\n",
    "|   6    |   0.257123   |  0.254316  |   89.50   |  238.22 |\n",
    "|   7    |   0.247575   |  0.248300  |   89.76   |  223.09 |\n",
    "|   8    |   0.239426   |  0.243885  |   89.81   |  216.53 |\n",
    "|   9    |   0.233904   |  0.239794  |   90.08   |  210.70 |\n",
    "|  10    |   0.227885   |  0.236559  |   90.17   |  221.15 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('best_model_0.pkl', 'wb') as file:\n",
    "    pickle.dump(best_model_0, file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def test_model(model, test_dataloader, compare_data=False):\n",
    "    '''\n",
    "    test the model on test data, return a DataFrame \n",
    "    of ID, prediction label and actural label\n",
    "    '''\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    losses = []\n",
    "    # ids = []\n",
    "    # ids.to(device)\n",
    "    # predictions.to(device)\n",
    "    # labels.to(device)\n",
    "\n",
    "    for i, batch in enumerate(test_dataloader):\n",
    "\n",
    "        b_input, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input)\n",
    "\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        losses.append(loss.item())\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        # ids.extend(list(b_ids))\n",
    "        labels.extend(list(b_labels))\n",
    "        predictions.extend(list(preds))\n",
    "\n",
    "        if i % 200 == 0 and i != 0:\n",
    "            print(\"After {} batches, the loss is {}\".format(i, loss))\n",
    "\n",
    "\n",
    "    test_loss = np.mean(losses)\n",
    "\n",
    "    # if we need to check predictions and labels, \n",
    "    # return test_loss and a pred-label dataframe\n",
    "\n",
    "    if compare_data:\n",
    "        performance = {\"prediction\": predictions,\n",
    "                       \"label\": labels}\n",
    "        return test_loss, pd.DataFrame(performance)\n",
    "    \n",
    "    # if we only need to check model performance, \n",
    "    # return test_loss and confusion matrix\n",
    "    \n",
    "    else:\n",
    "        cm = confusion_matrix(labels, predictions)\n",
    "        return test_loss, cm\n",
    "\n",
    "def eval_cm(cm):\n",
    "    '''\n",
    "    ravel a comfusion matrix, and return \n",
    "    accuracy, precision, recall, f1 \n",
    "    '''\n",
    "    tn, fp, fn, tp = cm.ravel() \n",
    "    accu = (tp + tn) / (tp + tn + fp + fn)\n",
    "    prec = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * (prec * recall) / (prec + recall)\n",
    "\n",
    "    eval = {\"Accuracy\": [accu],\n",
    "            \"Precision\": [prec],\n",
    "            \"Recall\": [recall],\n",
    "            \"F1\": [f1]}\n",
    "    \n",
    "    return pd.DataFrame(eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, cm = test_model(best_model_0, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.914455</td>\n",
       "      <td>0.920194</td>\n",
       "      <td>0.910714</td>\n",
       "      <td>0.91543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Precision    Recall       F1\n",
       "0  0.914455   0.920194  0.910714  0.91543"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_cm(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_dataloader):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's\n",
    "    performance on our validation set.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    test_accuracy = []\n",
    "    test_loss = []\n",
    "\n",
    "    for batch in test_dataloader:\n",
    "        b_input, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input)\n",
    "\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        test_loss.append(loss.item())\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        test_accuracy.append(accuracy)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    test_loss = np.mean(test_loss)\n",
    "    test_accuracy = np.mean(test_accuracy)\n",
    "\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6998165502894524, 50.59357541899441)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(best_model_og, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "with open('best_model_og.pkl', 'rb') as file:\n",
    "    best_model_og = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6998958001452044, 50.981404958677686)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(best_model_og, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_accuracy = evaluate_val(best_model_og, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50.981404958677686"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capp30255",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
